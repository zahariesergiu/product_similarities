{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d2fd8d-249b-49ec-8b24-160909b9730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertLMHeadModel, BertForMaskedLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00267dea-1eac-481c-8c13-a883e6e22937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dumitrescustefan/bert-base-romanian-cased-v1 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer and model using bert romanian cased model\n",
    "# https://github.com/dumitrescustefan/Romanian-Transformers\n",
    "# https://huggingface.co/dumitrescustefan/bert-base-romanian-cased-v1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "model = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c96e01d8-7900-4843-8d67-a39a9fff48e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dumitrescustefan/bert-base-romanian-uncased-v1 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at dumitrescustefan/bert-base-romanian-uncased-v1 were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at dumitrescustefan/bert-base-romanian-uncased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer and model using bert romanian cased model\n",
    "# https://github.com/dumitrescustefan/Romanian-Transformers\n",
    "# https://huggingface.co/dumitrescustefan/bert-base-romanian-cased-v1\n",
    "\n",
    "BERT_RO_UNCASED = \"dumitrescustefan/bert-base-romanian-uncased-v1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_RO_UNCASED)\n",
    "\n",
    "model = AutoModel.from_pretrained(BERT_RO_UNCASED)\n",
    "model2 = BertLMHeadModel.from_pretrained(BERT_RO_UNCASED)\n",
    "model3 = BertForMaskedLM.from_pretrained(BERT_RO_UNCASED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3376ceb-edb0-4652-b4c1-44f3581781f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7d578a-6435-4df9-b3d6-06531c2b7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(model, data):\n",
    "    embeddings = []\n",
    "    for doc in data:\n",
    "        _tokens = tokenizer.encode(doc, add_special_tokens=True, return_tensors=\"pt\")\n",
    "#         for k in _tokens:\n",
    "#             print(tokenizer.decode(k))\n",
    "        _tokens_embeddings = model(_tokens)[0]\n",
    "        _tokens_embeddings_mean = np.mean(_tokens_embeddings.detach().numpy().squeeze(), axis=0)\n",
    "        embeddings.append(_tokens_embeddings_mean)\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe6cea-0ff6-433b-a4d1-55f2dab738b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = create_embeddings(model, data.description)\n",
    "clean_embeddings = create_embeddings(model, data.clean_description)\n",
    "clean_embeddings2 = create_embeddings(model, data.clean_description2)\n",
    "clean_embeddings3 = create_embeddings(model, data.clean_description3)\n",
    "clean_embeddings4 = create_embeddings(model, data.clean_description4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ed4e8-466e-4fbb-8d92-f9e6afa1a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_matrix(embeddings, metric=cosine_distances):\n",
    "    x = pd.DataFrame(metric(embeddings, embeddings))    \n",
    "    x =x.round(decimals=2)\n",
    "    \n",
    "    mask = np.triu(x.corr())\n",
    "    mask = None\n",
    "\n",
    "    fig, ax = pyplot.subplots(figsize=(15,15))\n",
    "    ax.hlines([3, 6, 9], *ax.get_xlim())\n",
    "\n",
    "    cmap = sns.cm.rocket_r\n",
    "    sns.heatmap(x, linewidth=0.5, annot=True, ax=ax, cmap=cmap, mask=mask)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171c855-aecc-467f-8509-343fd6dac192",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_similarity_matrix(embeddings, euclidean_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65b05e-4a72-40d8-a36c-f8eace7968a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_similarity_matrix(embeddings, cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db82362-1a36-4be0-bfea-dbed9078e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_similarity_matrix(clean_embeddings, cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c532500-7126-497c-9906-068b5b04e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_similarity_matrix(clean_embeddings2, cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d77d0-1b79-495f-94d2-5e27ff01b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_similarity_matrix(clean_embeddings3, cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cf6db-3a98-4aa0-a711-7cb88bf0a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_similarity_matrix(clean_embeddings4, cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe22dea-06a6-4bbb-8b45-a41c7bc8ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [\"pizza\"]*5 + [\"paste\"]*5 + [\"book\"]*5 + [\"dipers\"]*5\n",
    "\n",
    "for _embeddings in [embeddings, clean_embeddings, clean_embeddings2, clean_embeddings3, clean_embeddings4]:\n",
    "    acc = 0\n",
    "    for embedding, label in zip(_embeddings, true_labels):\n",
    "        acc += (data.iloc[np.argsort(cosine_distances([embedding], _embeddings)).squeeze()].category[:5] == label).sum()/5\n",
    "    #     print(acc)\n",
    "    acc = acc/len(embeddings)\n",
    "    print(acc)  # We get top5 sinmilarity with an accuracy of 79%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88031e9c-52ea-40d9-9802-a5d7382d9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingslm = create_embeddings(model2, data.description)\n",
    "clean_embeddingslm = create_embeddings(model2, data.clean_description)\n",
    "clean_embeddingslm2 = create_embeddings(model2, data.clean_description2)\n",
    "clean_embeddingslm3 = create_embeddings(model2, data.clean_description3)\n",
    "clean_embeddingslm4 = create_embeddings(model2, data.clean_description4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de875d-79fa-4381-9dc8-ad60b96e2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [\"pizza\"]*5 + [\"paste\"]*5 + [\"book\"]*5 + [\"dipers\"]*5\n",
    "\n",
    "for _embeddings in [embeddingslm, clean_embeddingslm, clean_embeddingslm2, clean_embeddingslm3, clean_embeddingslm4]:\n",
    "    acc = 0\n",
    "    for embedding, label in zip(_embeddings, true_labels):\n",
    "        acc += (data.iloc[np.argsort(cosine_distances([embedding], _embeddings)).squeeze()].category[:5] == label).sum()/5\n",
    "    #     print(acc)\n",
    "    acc = acc/len(embeddings)\n",
    "    print(acc)  # We get top5 sinmilarity with an accuracy of 79%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5bf8e-0adc-4125-83b1-4fdf18bdd3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_similarity_matrix(clean_embeddingslm3, cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd42698-1bb0-483e-834f-2db1bdb421d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3 = BertForMaskedLM.from_pretrained(BERT_RO_UNCASED)\n",
    "\n",
    "# embeddingsmlm = create_embeddings(model3, data.description)\n",
    "# clean_embeddingsmlm = create_embeddings(model3, data.clean_description)\n",
    "# clean_embeddingsmlm2 = create_embeddings(model3, data.clean_description2)\n",
    "# clean_embeddingsmlm3 = create_embeddings(model3, data.clean_description3)\n",
    "# clean_embeddingsmlm4 = create_embeddings(model3, data.clean_description4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d973b1-6fe6-4589-bc17-adcaa6b4d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_labels = [\"pizza\"]*5 + [\"paste\"]*5 + [\"book\"]*5 + [\"dipers\"]*5\n",
    "\n",
    "# for _embeddings in [embeddingsmlm, clean_embeddingsmlm, clean_embeddingsmlm2, clean_embeddingsmlm3, clean_embeddingsmlm4]:\n",
    "#     acc = 0\n",
    "#     for embedding, label in zip(_embeddings, true_labels):\n",
    "#         acc += (data.iloc[np.argsort(cosine_distances([embedding], _embeddings)).squeeze()].category[:5] == label).sum()/5\n",
    "#     #     print(acc)\n",
    "#     acc = acc/len(embeddings)\n",
    "#     print(acc)  # We get top5 sinmilarity with an accuracy of 79%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b7394-fbfe-47f6-83dd-a7215afa4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = get_similarity_matrix(clean_embeddingsmlm3, cosine_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "product_similarities",
   "language": "python",
   "name": "product_similarities"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
